
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Surabhi Gaopande's Personal Site'</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- Le styles -->
<link href="css/bootstrap.css" rel="stylesheet">
<link href="css/extra_style.css" rel="stylesheet">
<!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
<!--[if lt IE 9]>
<script src="//html5shim.googlecode.com111111/svn/trunk/html5.js"></script>
<![endif]-->

<!-- Google Analytics code -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60339627-1', 'auto');
  ga('send', 'pageview');

</script>

</head>
<body>
	
	<div id="fb-root"></div>
	<script>(function(d, s, id) {
	  var js, fjs = d.getElementsByTagName(s)[0];
	  if (d.getElementById(id)) return;
	  js = d.createElement(s); js.id = id;
	  js.src = "http://connect.facebook.net/en_US/all.js#xfbml=1";
	  fjs.parentNode.insertBefore(js, fjs);
	}(document, 'script', 'facebook-jssdk'));</script>

	<div class="container">
	<!-- Main hero unit for a primary marketing message or call to action -->
	<div class="leaderboard">
	<div class="row">
		<div class="span4">
		<img src="images/sam_horiz.jpg"  >
		</div>
	<div class="span7" id="title">
	<h1>Surabhi Gaopande</h1>
	<p> Smarter Systems for Smarter Tomorrow!</p>
	</div>
	</div>
	</div>
	<hr>
<!-- Example row of columns -->
<div class="row">
	
	
	
	
	
	<div class="span2">
		
		<div class="nav-links">
			<ul class="nav nav-pills nav-stacked">
				<li><a href="index.html"><i class="icon-home"></i> Home ></a></li>
				<li><a href="research.html"><i class="icon-road"></i> Research ></a></li>
				<li><a href="personal.html"><i class="icon-user"></i> Personal ></a></li>
				<li><a href="contact.html"><i class="icon-comment"></i> Contact ></a></li>
			</ul>
		</div> <!-- /navlinks-->
	</div> <!--/span-->
	
	<div class="span10">

	<div class="row">
		<div class="span6">
      		<h2>Statistical approach for distance estimation using Inverse Perspective Mapping on embedded platform</h2>
      		<strong><p>The algorithm that smart phones use to point distance of remote objects! Or the mechanism that makes the driving assistance systems robust. </p>
<br>
      		A mono vision distance estimation algorithm which can be used for various applications such as obstacle avoidance in case of an autonomous slow moving vehicle (ASMV) has been devised. 
			The accurate estimation of the distance of the obstacle from camera position was determined using statistical analysis. An accuracy of about 90 % was achieved. 
			The results obtained with statistical approach enhance the performance of the system and make it ideal for real time processing of closed loop obstacle avoidance control
			mechanism of an ASMV. 
				<br> 
			<br>
The accuracy achieved is very high and nearly equals that obtained by stereo vision mechanism. The added advantage of the proposed algorithm is that
			no prior information is needed about the object to be detected and calibration needs to be just once during system installation for distance estimation for a particular environment. 
			The algorithm also takes care of non-ideal behavior of the camera lens and tries to eliminate it by carrying out un-distortion. The adaptive thresholding mechanism 
			eliminates need of a fixed threshold value, thereby addressing an important problem of most of the thresholding algorithms used in image processing.
			Considering the multiple constraints of mono-vision distance estimation approach, the proposed algorithm can be thought as a low cost and efficient solution to the problem.
					<br>
			<br>
			</p>
			<p><strong><u>Publications</strong></u></p>
			<p> <strong> <a href="http://ieeexplore.ieee.org.ezproxy.lib.vt.edu/xpls/icp.jsp?arnumber=7030430">
			Statistical approach for distance estimation using Inverse Perspective Mapping on embedded platform</strong></a><br>   
		
    	</div><!--/span-->

		<div class="span4">
					<img class = "carousel_img offset1" src="images//thumbs/p4.JPG" alt="">
		</div>
		
			
      		
	</div><!--/row-->
	
	<br>
	<br>
	<div class="row">
		
    	<div class="span6">
      		<h2>Effects of Personalized Human-Robot Interaction in Educational Tutoring</h2>
      		<p><strong>How can Social Robots most effectively tutor students to help learn problem-solving skills?</strong> Collaboration with <a href="http://www.danleyzberg.com">Dan Leyzberg</a>.</p>
			
			<br>
			<p> We investigated the efficacy of three different methods of tutoring instruction: verbal instruction, video agent, and embodied robot and found that, even when delivering identical content, students who received instruction from an embodied robot learned a puzzle task significantly more quickly than students in either of the other two cases. 
				<br>
				<br>
				In addition, we developed a method for tracking a subject&#39;s proficiency at a complicated cognitive task composed of many unique skills via indirect observation. Rather than prompting a student to demonstrate his/her proficiency at each skill for evaluation, the tutor generalizes probabilistic estimates of proficiency from observed gameplay, without interrupting the learning task.
				<br>
				<br>
				Finally, we isolated the effect of <em>personalized</em> tutoring in an HRI study and found that even simple personalizations could produce significant learning gains during a 1hr. experiment</p>
				
				<p><strong><u>Publications</strong></u></p>

				<p> <strong> <a href="resources/cog_sci_12_final.pdf">The Physical Presence of a Robot Tutor Increases Cognitive Learning Gains</strong></a><br>  Leyzberg, D., <b>Spaulding, S.</b>, Toneva, M. and Scassellati, B. <i>34th Annual Meeting of the Cognitive Science Society (Cog Sci ’12), July 2012.</i></p>

				<p> <strong> <a href="resources/hri_14_final.pdf">Personalizing Robot Tutors to Individuals' Learning Differences</strong></a><br>  Leyzberg, D., <b>Spaulding, S.</b>, and Scassellati, B.<i> 9th ACM/IEEE Conference on Human-Robot Interaction (HRI ’14), March 2014.</i></p>
      		
    	</div><!--/span-->
		<div class="span4">
				<img class = "carousel_img offset1" src="images/keepon_from_movie.png" alt="">
		</div>

	</div><!--/row-->
	
	</div> <!--/span-->
</div> <!-- row -->
<footer>
</footer>
</div> <!-- /container -->

<!-- Project Page Divs-->


<div id="CoReader" class="modal hide fade" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
  <div class="modal-header">
    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
    <h3 id="myModalLabel">A Co-Reading Robot Companion to Help Improve Child Literacy</h3>
  </div>
  <div class="modal-body">
	<img class = "carousel_img offset1" src="images/dragonbot.jpeg" alt="" height=75% width=75%><br><br>

	 <br> <p> <strong>Abstract:</strong> We are working to develop a robotic learning companion for childhood education. Recent work has highlighted the importance of simple word exposure in developing literacy skills. Unfortunately for many children, especially those from low-income or immigrant families, parents are sometimes too busy or incapable of providing the exposure to words that is critical to literacy success. The vocabulary size difference between children of professionals and children growing up in poverty has been estimated to be as high as 
		<a href="http://www.theatlantic.com/technology/archive/2010/03/the-32-million-word-gap/36856/"> 32 million. </a>
		<br>
		<br>
		In partnership with education specialists, we are currently working to develop an autonomous robotic platform that can be used to supplement parental co-reading and improve the quantity and quality of words that young children are exposed to. Our system will initially be teleoperated by trained education specialists, with the goal of training an autonomous behavior policy that will emulate co-reading best practices and provide an exciting, educational interaction. </p> 
	<p><strong> Tech Involved: </strong> Dragonbot robot, Kinect SDK, All other code written in Java. </p> 
		
  </div>
  <div class="modal-footer">
    <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
  </div>
</div>



<div id="CogTutor" class="modal hide fade" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
  <div class="modal-header">
    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
    <h3 id="myModalLabel">Social Robotics and Intelligent Cognitive Tutoring</h3>
  </div>
  <div class="modal-body">
	<img class = "carousel_img offset1" src="images/keepon_from_movie.png" alt="" height=75% width=75%><br><br>
	
	<p> <strong> <a href="resources/cog_sci_12_final.pdf">Social Robotics and Intelligent Cognitive Tutoring: </strong></a><br>  <i>Leyzberg, D., Spaulding, S., Toneva, M. and Scassellati, B.</i> <b>“The Physical Presence of a Robot Tutor Increases Cognitive Learning Gains,”</b> 34th Annual Meeting of the Cognitive Science Society (Cog Sci ’12) July 2012.</p>
	
	<p> <strong> <a href="resources/cog_sci_12_final.pdf">Social Robotics and Intelligent Cognitive Tutoring: </strong></a><br>  <i>Leyzberg, D., Spaulding, S., Toneva, M. and Scassellati, B.</i> <b>“The Physical Presence of a Robot Tutor Increases Cognitive Learning Gains,”</b> 34th Annual Meeting of the Cognitive Science Society (Cog Sci ’12) July 2012.</p>
	 <br> <p> <strong>Abstract:</strong> We investigated the efficacy of three different methods of tutoring instruction: verbal instruction, video agent, and embodied robot and found that, even when delivering identical content, students who received instruction from an embodied robot learned a puzzle task significantly more quickly than students in either of the other two cases. In addition, we developed a method for tracking a subject&#39;s proficiency at a complicated cognitive task composed of many unique skills via indirect observation. Rather than prompting a student to demonstrate his/her proficiency at each skill for evaluation, the tutor generalizes probabilistic estimates of proficiency from observed gameplay, without interrupting the learning task. This aspect is particularly noteworthy since each observable action may correspond to one, many, or no relevant skills, thus developing a model of the internal state of the student is a non-trivial and important task </p> 
	<p><strong> Tech Involved: </strong> Keepon robot. All other code written in Java for <a href=www.processing.org> Processing </a>. </p> 
	<p> The full paper is available <a href=resources/cog_sci_12_final.pdf> here </a>
		
  </div>
  <div class="modal-footer">
    <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
  </div>
</div>


<div id="Trust" class="modal hide fade" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
  <div class="modal-header">
    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
    <h3 id="myModalLabel">How do Social Behaviors Influence a Participant's Willingness to Take Bad Advice in a Game?'</h3>
  </div>
  <div class="modal-body">
	<p><a href=\resources\Decision_making.pdf> <strong>Robot Trust and Irrational Decision Making: </strong> </a></p> <br> <p> <strong>Abstract:</strong> We recorded gameplay data from a unique card game and ratings from a post-experiment survey from 20 participants, consisting of. During each of 12 rounds of gameplay, participants received advice from a robotic partner that they could choose to accept or ignore. <br><br>  We devised a 2-by-2 experiment, in which participants were split into four groups based on two factors: the robot was either social or nonsocial, and it gave either consistently bad advice or gradually transitioned from good to bad advice. We tracked the pattern in which each participant accepted the robot’s advice. Furthermore, in the post-experiment survey, participants were asked to rate how human-like, trustworthy, and likeable the robot seemed. <br><br> Our results suggest that social behaviors strongly influence participants’ level of trust, but that these same features cause a robot to appear more fallible. Participants in the social cases were less likely to allow the advice to affect their gameplay. We further found that participants perceive a nonsocial robot’s behavior as relatively static and a social robot’s as relatively variable. Our results highlight the importance of a robot’s social engineering on building trust with humans. </p> <p><strong> Tech Involved: </strong> 	Aldebaran Nao SDK, Choreographe. All other code written in Python. </p> <p> The full paper is available <a href=resources/Decision_making.pdf> here </a>
  </div>
  <div class="modal-footer">
    <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
  </div>
</div>

<div id="Semantics" class="modal hide fade" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
  <div class="modal-header">
    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
    <h3 id="myModalLabel">How can robots use unstructured natural language to learn about objects in the world?</h3>
  </div>
  <div class="modal-body">
	<p> <strong>Natural Language Inference of Object Sentiment: </strong></p> <br> <p> <strong>Abstract:</strong> Within Social Robotics there has been a great deal of research investigating non-verbal methods of communication. Computational analysis of body posture, gaze, and often direct input via a software interface are commonly leveraged to create appropriate social responses in robotic systems. Even in systems that do make use of verbal behavior, semantic information is typically discarded in favor of prosodic interpretation. In the real world, however, the semantics of speech contain a great deal of information about what we mean, yet there is relatively little work that leverages this rich source of meaning. This project aims to allow a robot to incorporate semantic information into a learning task.
		
		
		<br><br> For my senior thesis, I developed a robotic system that recognizes the object of natural language sentences and uses the semantic content of a conversation to calculate a sentiment score for it. The robot's behavior will then be determined by the score of the object of discussion.
		<br><br> Users interact with a robot in a series of 'teaching' and 'discussion' phases. In the teaching phase, the robot learns to recognize the object and builds up a sentiment for the object from the user's verbal input. In the discussion phase, the robot responds to prompts from the user (e.g. 'How do you feel about carrots?') with physical behavior determined by the sentiment score. </p> <p><strong> Tech Involved: </strong> Aldebaran Nao, NLTK software package, OpenCV, Dragon Dictate Speech Recognition Software. All other code written from scratch for Python</p>
  </div>
  <div class="modal-footer">
    <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
  </div>
</div>


<!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="js/jquery.js"></script>
    <script src="js/bootstrap-transition.js"></script>
    <script src="js/bootstrap-alert.js"></script>
    <script src="js/bootstrap-modal.js"></script>
    <script src="js/bootstrap-dropdown.js"></script>
    <script src="js/bootstrap-scrollspy.js"></script>
    <script src="js/bootstrap-tab.js"></script>
    <script src="js/bootstrap-tooltip.js"></script>
    <script src="js/bootstrap-popover.js"></script>
    <script src="js/bootstrap-button.js"></script>
    <script src="js/bootstrap-collapse.js"></script>
    <script src="js/bootstrap-carousel.js"></script>
    <script src="js/bootstrap-typeahead.js"></script>
    <script src="js/bootstrap-affix.js"></script>

    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/google-code-prettify/prettify.js"></script>

    <script src="assets/js/application.js"></script>
</body>
</html>
